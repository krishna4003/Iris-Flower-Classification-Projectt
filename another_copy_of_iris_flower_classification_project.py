# -*- coding: utf-8 -*-
"""Another copy of Iris Flower Classification Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vJ-QXytqJkyI8hiJPTXfkH5TfYB9HGDe

# !!Iris Flower Classification Project
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df=pd.read_csv('/content/Iris.csv')

df

x=df.iloc[:,1:5].values #separation of dependent variables

x

y=df.iloc[:,5].values # separation of independent variable

y

df.isnull().sum() # checking for null values

from sklearn.preprocessing import LabelEncoder # label encoding

label_encoder_x=LabelEncoder()

y=label_encoder_x.fit_transform(y)

y

from sklearn.preprocessing import OneHotEncoder #one hot encoding

one_hot_encoder=OneHotEncoder()

x= one_hot_encoder.fit_transform(df.Species.values.reshape(-1,1)).toarray()

x

from sklearn.model_selection import train_test_split #training and testing dataset into test and train set

x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3, random_state=200)

x_test

y_test

len(x_test)

len(x_train)

from sklearn.neighbors import KNeighborsClassifier # applying KNN algorithm

clf= KNeighborsClassifier(n_neighbors=10)

clf.fit(x_train, y_train)

clf.predict(x_test)

clf.score(x_test,y_test) #accuracy is 100%

#GridSearchCv
#importing  necessary libraries
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

model=KNeighborsClassifier() #loading KNN model

parameters={                             #hyperparameters
    'n_neighbors':[ 5,10,67,90],
    'p':[3,10,20]
}

#grid search
classifier=GridSearchCV(model,parameters,cv=7)

#fitting the data to our model
classifier.fit(x,y)

classifier.cv_results_

#best parameters
best_p=classifier.best_params_
print(best_p)

#highest accuracy
highest_a=classifier.best_score_
print(highest_a)

#loading the results to pandas dataframe
result=pd.DataFrame(classifier.cv_results_)
result

grid=result[['param_n_neighbors', 'param_p','mean_test_score']]
grid

#Model Selection
 from sklearn.linear_model import LogisticRegression
 from sklearn.svm import SVC
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.neighbors import KNeighborsClassifier

#list of models
models_list=[ LogisticRegression(),SVC(),RandomForestClassifier(), KNeighborsClassifier()]

#creating a dictionary that contains hyperparameters values for the above  mentioned models
model_hyperparameters={
    'log_reg_hyperparameters':{
        'C':[1,5,10,20]
    },
    'svc_hyperparameters':{
        'kernel':['linear','poly','rbf','sigmoid'],
        'C':[20,54,3,2]
    },
    'knn_hyperparameters':{
        'n_neighbors':[10,34,20]
    },
    'random_forest_hyperparameters':{
        'n_estimators':[10,20,40,100]
    }



}

print(model_hyperparameters.keys())

model_keys=list(model_hyperparameters.keys())
model_keys

model_hyperparameters[model_keys[2]]

#Applying Grid SearchCv
def model_selection(list_of_models,hyperparameters_dict):
  result=[]
  i=0
  for models in list_of_models:
    key=model_keys[i]
    params=hyperparameters_dict[key]
    i+=1

    print(models)
    print(params)
    classifier=GridSearchCV(models,params,cv=10)

    #fitting the data to classifier
    classifier.fit(x,y)
    result.append({
        'model used':models,
        'highest score':classifier.best_score_,
        'best hyperparameters':classifier.best_params_
    })
  result_dataframe=pd.DataFrame(result,columns=['model used','highest score', 'best hyperparameters'])
  return result_dataframe

model_selection(models_list, model_hyperparameters)